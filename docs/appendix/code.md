# code编码  
## 二进制   
只有0和1，没有中间状态  
继电器是最初可以表示二进制的工具  
有真和假两种，也可用1和0表示，只是两种方式而已，为什么用二进制?因为要尽可能区分开各种信号，而且当时数学领域刚好有一整个数学分支的存在，专门处理真和假，它已经解决了所有的法则和运算，叫布尔代数  
布尔代数有三个基本操作NOT AND OR  
	
NOT，非门，有输入和输出，not会把0和1反转，0变成1，1变成0。  
	
AND是与门，是有两个输入，一个输出，只有两个都1才会一，否则都0  
	
或门，OR，最容易理解的  
	
还有一个异或XOR   
可以用与或非门来组成异或  
	
想表示更多的信息，加位数就行了  
二进制也有位数，但不是十位，百位，而是二位，四位，八位  
而8位的意思是有8个位:eg：10110101，最高可表示到2的八次方即256，每一个数字就是一个信息，比如代表一种颜色  
8位机，8位音乐，都代表计算机都是8位8位处理的  
	
8位太常见了，所以干脆用字节表示，一字节等于8位，如果有10字节（B），代表有80位  
千字节（KB），兆字节（MB）（1兆）千兆字节（GB），1TB=1.1 x 10¹²B  
门只用来计算，不用来表示位  
有另一种表示方法，1千字节=2的10次方=1024字节  
32位和64位计算机都是这种说法     
表示正负一般是用第一位数的1和0代表  
非整数，小数用浮点数表示，没必要弄懂  
表示字母时，用数字给字母编号即可  
但不同国家的编码不同，在美国打开中国邮件会乱码   
Unicode统一了所有编码的标准，所有国家的所有字符，一共占了12万个位置  
格式就是用来识别数字的，eg：声音，颜色，字母，  
	
知识的高度抽象化让我们不用在物理层面考虑这么多，把他们抽象就行了，因为你没必要考虑异或，只需要知道有这个东西即可  

	
## 逻辑和开关，门  
有真和假两种，也可用1和0表示，只是两种方式而已，为什么用二进制?因为要尽可能区分开各种信号，而且当时数学领域刚好有一整个数学分支的存在，专门处理真和假，它已经解决了所有的法则和运算，叫布尔代数  
布尔代数有三个基本操作NOT AND OR  
NOT，非门，有输入和输出，not会把0和1反转，0变成1，1变成0。酷的是晶体管刚好可以完成这个任务，晶体管有三根线，2根电极一根控制线，控制线通电，电流就会从一个电极流到另一个，就像水龙头，  
	
控制线为input，下面为output，这样没有任何用  
	
如果反过来，上面作为输出，而下面接地，输入为1时输出就为0，反之亦然  
AND是与门，是有两个输入，一个输出，只有两个都1才会一，否则都0  
	
	
或门，OR，最容易理解的  
	
可以用与或非门来组成异或  
	
知识的高度抽象化让我们不用在物理层面考虑这么多，把他们抽象就行了，因为你没必要考虑异或，只需要知道有这个东西即可  
	
## 如何实现加法，减法  
	
有了加法，就可以实现减法，乘法和除法   
实现加法要用半加器（加减两个一位数用的机器，两个数为输入，进位和结果是输出)  
通过半加器可以组装成全加器  
这是数电的知识，一个八位的加法器就要用到144个继电器，如果全部展示这些，你一定会崩溃的    
还有波纹进位加法器，可以加快加法的速度  
	
是由算数逻辑单元（ALU）做成的  
半加器  
两个输入，A和B  
两个输出，总和和进位  
用两个半加器制成全加器  
用8个全加器制作8位加法器  
如果第九位有进位，那就会溢出  
现在计算机用的是超前进位加法器  
	
计算机实现减法通常使用补码运算。在计算机中，减法可以通过将减数取补码，然后与被减数进行加法运算来完成。这意味着将减数的每一位取反（0变为1，1变为0），然后加上1。例如，计算 7 - 3，可以按照以下步骤完成：  
	
3的补码是自身（因为3是正数）：0000 0011  
取3的补码（0000 0011）的反码：1111 1100  
反码加1得到3的补码：1111 1101  
7的补码为0000 0111  
将7的补码（0000 0111）和3的补码（1111 1101）进行加法运算：  
0000 0111  
1111 1101  
0000 0100  
	
所以，7减3等于4。计算机中的减法本质上转化为加法和补码运算。  
	
	
在电子工程领域中，反馈和触发器都是重要的概念，但它们指代不同的概念和功能。  
	
***
## 反馈与触发器  
1. 反馈（Feedback）：在电子系统中，反馈是指将系统输出的一部分信号（或信息）返回到系统输入或其他部分，以对系统的性能、稳定性或其他特征进行控制和调节的过程。反馈可以是负反馈或正反馈。负反馈通过将一部分输出信号与输入信号相减来调节系统，以维持系统的稳定性和准确性。而正反馈则会增强输入信号，可能导致系统不稳定甚至失控。  
	
2. 触发器（Flip-Flop）：在数字电路中，触发器是一种用来存储和处理二进制信息的电路元件。它能够在特定时钟信号的作用下，在给定的条件下保持或改变其输出状态。常见的触发器包括SR触发器、D触发器、JK触发器和T触发器等，它们在数字系统中用于存储数据、实现时序逻辑等操作。  
	
在电子工程中，反馈和触发器都是设计和控制电路系统时至关重要的概念。反馈帮助调节系统性能和稳定性，而触发器则是用来存储和处理数字信息的关键组件之一。  
主要类型的触发器：  
SR触发器：使用S（Set）和R（Reset）输入来控制其状态。当S=1、R=0时，输出为高电平；当S=0、R=1时，输出为低电平；当S=0、R=0时，保持前一个状态；当S=1、R=1时，处于不稳定状态。  
	
D触发器：只有一个数据输入（D），在时钟的作用下，将输入数据存储到触发器中，并在时钟脉冲的边沿或电平上升/下降时改变输出。  
	
JK触发器：具有三个输入：J、K和时钟。它可以模拟SR触发器的功能，并且在某些输入条件下能够避免不稳定状态。  
	
T触发器：只有一个输入（T），它被称为切换输入。当时钟信号出现时，T触发器在每个时钟脉冲到来时将输出状态进行切换。  
	
	
	
***
## 字节和十六进制  
由于字节比位常用，所以二进制有点不够用，所以就发明了16字节
	
	
	
## 储存器组织  
有寄存器和内存  
内存RAM，只能暂时储存  
memory是可以记忆的  
寄存器（Register）：  
定义：寄存器是一种高速存储设备，位于CPU（中央处理器）内部，用于临时存储指令、数据或地址。它们是CPU内部的存储单元，速度非常快，用于暂时保存和处理数据。  
	
特点：寄存器的容量非常有限，一般以位（bit）为单位计量。它们直接参与计算机指令的执行，因此速度非常快，但是由于容量有限，通常用于存储临时数据、计算结果和指令操作数。  
	
用途：在CPU执行指令时，需要暂时存储数据或指令，寄存器起到了临时存储、数据传输和运算等作用。  
	
内存（Memory）：  
定义：内存是计算机中用于存储程序、数据和指令的设备，它通常分为RAM（随机存取存储器）和ROM（只读存储器）两种类型。内存的容量较大，相对于寄存器来说速度较慢。  
	
特点：内存的容量比寄存器大得多，可以存储更多的数据和程序。RAM是易失性存储器，数据在断电时会丢失；而ROM是非易失性存储器，其中的数据在断电时不会消失。  
	
用途：内存用于存储正在运行的程序、操作系统、应用程序和用户数据。CPU通过从内存读取数据和指令，执行计算机程序。  
	
关系：  
寄存器和内存都是计算机系统中存储数据的关键组件，但寄存器位于CPU内部，速度非常快，用于临时存储数据；而内存则是外部存储设备，容量较大，用于存储程序和数据。  
	
CPU通过在寄存器中暂存数据来进行高速运算，并在需要时与内存交换数据，以执行计算机指令。且  
	
寄存器和内存都是计算机系统中存储数据的不同层次，寄存器作为CPU的一部分，速度快但容量有限；而内存作为主要的存储介质，容量大但速度相对较慢。  
	
	
## 自动操作  
人类很懒，所以要自动的东西来做一些事情，这些东西就是程序  
一台计算机主要由四方面组成  
处理器，存储器，输入，输出  
除了上述，还有中央处理单元(central procesaing unit）简称CPU  
算数逻辑单元（arithmetic logic unit）ALU  
输入到存储器中的指令和数值叫做软件  
软件和计算机程序的意思基本上是等价的  
	
	
## 从算盘到芯片  
1.计算机就是用来计算的，比如统计人口，就是算盘的原理，十个就进一个，就是计数加统计加储存，  
2.最开始是用机械继电器，目的是模拟0和1，用0和1两种状态代表信息，这叫二进制(binary)但太重，太慢，机械会生锈  
然后用电子真空管，就是二极管，从中间发电子，旁边的接住，所以是单向，且从机械跨越到了电子，还是慢，也容易坏，特别巨大，  
就是有虫子在上面，所以有了bug这个名词  
最后发明了晶体管，特别复杂，有量子力学的概念，就是通过半导体来模拟0和1，现在最小的晶体是50纳米，一张纸是十万纳米，晶体管不但小，还计算的特别快，所以才有了今天的计算机  
	
## 两种典型的微处理器  
将中央处理器的所有构件封装到硅芯片上，就得到了微处理器  
Intel 8080 和 Intel 6800  
微处理器是一种集成在单个芯片上的中央处理器（CPU）。它是计算机系统中的核心部件，负责执行各种指令并控制数据的处理。  
	
主要特点和功能：  
集成度高：微处理器集成了多个功能单元（如算术逻辑单元、控制单元、寄存器等），将这些功能集成到一个芯片上，因而被称为集成电路。  
	
指令执行：微处理器能够解析并执行计算机程序中的指令，包括算术运算、逻辑运算、数据传输、控制流程等操作。  
	
时钟控制：微处理器内部有时钟信号控制其操作，时钟信号的脉冲决定了微处理器的工作速度和时序。  
	
多用途性：微处理器可用于不同类型的计算机和设备，从个人计算机、智能手机到嵌入式系统和其他各种电子设备。  
	
架构和指令集：不同厂商的微处理器有不同的架构和指令集，如x86架构（常见于Intel和AMD处理器）、ARM架构等。  
	
多核处理器：现代微处理器往往是多核的，即在同一芯片上包含多个处理核心，以提高处理能力和效率。   
	
微处理器是计算机系统中的关键部件，其性能和功能对整个系统的运行效率和能力有着重要影响。它承担着指令解析、数据处理、控制操作等任务，是现代计算机技术的基础  
	
字符到ASCII码：  
	
计算机使用ASCII码表示字符。例如，字母"A"对应的ASCII码是65，字母"a"对应的ASCII码是97，而数字"0"到"9"对应的ASCII码是48到57。  
ASCII码到字符：  
	
如果你有一个ASCII码的数字，你可以通过查找ASCII码表来找到对应的字符。例如，ASCII码为65的字符是大写字母"A"。  
	
## ASCII码和字符转换  
ASCII（American Standard Code for Information Interchange，美国信息交换标准代码）是一种常见的字符编码标准，用于将字符映射到数字（0-127）以便计算机进行识别和处理。在ASCII码中，每个字符对应一个唯一的数字。  
	
	
## 总线(bus)  
总线就是数字信号的综合  
有四种  
地址信号   
数据输出信号  
数据输入信号  
控制信号  
总线还可以供电  
	
	
	
***
	
## 操作系统  
操作系统，简称OS，其实也是一种程序  
但是它有操作硬件的特殊权限，可以运行和管理其他程序  
每个程序会占有一定的位置，所以切换到另一个程序的时候，不能丢失数据，解决办法:给每个程序建立专属内存块，会进行动态内存分配   
操作系统主要功能:  
处理器管理：操作系统负责管理和分配CPU资源，确保各个程序或进程按照一定的调度算法合理地使用CPU，以实现高效的运行。  
	
内存管理：操作系统管理计算机内存，包括内存分配、地址映射、内存释放以及虚拟内存的管理，以确保程序能够正确地加载、执行和结束。  
	
文件系统管理：操作系统负责管理计算机的文件系统，包括文件的创建、读取、写入、删除和修改，以及对文件进行组织、存储和保护。  
	
设备管理：操作系统管理各种设备，如硬盘、键盘、鼠标、打印机等外部设备，包括设备的驱动程序管理、设备的分配和控制，以及对设备进行有效地调度。  
	
用户界面：操作系统提供用户与计算机系统交互的界面，包括图形用户界面（GUI）和命令行界面（CLI），使用户能够操作和控制计算机系统。  
	
安全性和权限管理：操作系统确保计算机系统的安全性，通过权限控制和身份验证机制，管理用户对系统资源的访问权限，防止未经授权的访问和恶意操作。  
	
错误检测与处理：操作系统能够检测和处理系统中的错误和异常情况，包括硬件故障、程序错误或系统崩溃等，以确保系统的稳定性和可靠性。  
	
	
## 定点数和浮点数  
浮点数存储：浮点数采用特定的存储结构（按照IEEE 754标准），将符号位、指数部分和尾数部分组合成一个二进制数来存储。  
	
	
	
## 高级语言和低级语言  
用低级语言编写程序，既费力又费时，所以人们想出了高级语言来编程  
定义：高级语言是与特定应用或问题领域相关的抽象程度较高的语言，更接近人类自然语言，提供更高层次的抽象和更丰富的功能。  
	
特点：高级语言包括诸如Python、Java、C++等，其语法更容易理解，更接近人类自然语言，编写的代码更易读、易维护。  
	
优点：易学易用，可读性好，提高了编程效率，具有较好的可移植性。  
	
缺点：执行效率可能相对较低，对底层硬件的控制能力有所降低。  
	
高级语言更适合普通开发任务，提高了开发效率和代码的可读性，适用于快速开发、复杂项目和跨平台应用。  
	
低级语言更接近硬件层面，用于特定的硬件控制或性能要求高的场景，例如操作系统、嵌入式系统等。  
	
许多项目都采用高级语言进行开发，但在对性能要求极高或者需要直接控制硬件的场景下，低级语言则更为适用。  
	
	
	
	
图形化革命  
计算机的图形化革命是计算机科技发展的重要里程碑之一，它指的是计算机界面从字符界面（CLI，Command Line Interface）向图形用户界面（GUI，Graphical User Interface）的转变和发展。
	
图形化革命的重要事件和特点包括：  
图形用户界面的出现：1970年代末和1980年代初，Xerox PARC（帕克实验室）首先开发了图形用户界面（GUI），并引入了鼠标、窗口、图标和菜单等元素，这种界面更直观、易用，让用户能够通过点击和拖拽来进行操作。  
	
苹果Macintosh的推出：苹果在1984年推出了Macintosh计算机，它搭载了图形用户界面，采用了鼠标和可视化操作，成为当时革命性的产品。  
	
微软Windows的发展：微软推出了Windows操作系统，首先是Windows 1.0于1985年发布，之后不断发展，引入了图形化界面，使得PC用户也能够享受到类似Macintosh的用户体验。  
	
图形化界面的普及：图形用户界面的普及使得计算机的操作更为直观、易懂，用户不再需要记忆和输入复杂的命令，而是通过图形化的界面和操作来完成任务。  
	
图形处理和多媒体技术的进步：随着图形化革命的发展，图形处理和多媒体技术也得到了飞速发展，使得计算机能够处理和展示更丰富的图像、视频和音频内容。  
	
图形化革命改变了人们与计算机交互的方式，使得计算机技术更加普及和易用，从而推动了信息技术的迅速发展，并改变了人们的日常生活和工作方式。  
	
***	
